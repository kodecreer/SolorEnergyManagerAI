  env = GymEnv('SolarEnv-v0', device=device, frame_skip=frame_skip)
#     env = TransformedEnv(
#         env,
#         Compose(
#             # normalize observations
#             ObservationNorm(in_keys=["observation"]),
#             DoubleToFloat(
#                 in_keys=["observation"],
#             ),
#             StepCounter(),
#         ),
#     )
#     env.transform[0].init_stats(num_iter=1000, reduce_dim=0, cat_dim=0)
#     print("normalization constant shape:", env.transform[0].loc.shape)
#     print("observation_spec:", env.observation_spec)
#     print("reward_spec:", env.reward_spec)
#     print("done_spec:", env.done_spec)
#     print("action_spec:", env.action_spec)
#     # print("state_spec:", env.state_spec)
#     check_env_specs(env)

#     actor_net = nn.Sequential(
#     nn.LazyLinear(num_cells, device=device),
#     nn.Tanh(),
#     nn.LazyLinear(num_cells, device=device),
#     nn.Tanh(),
#     nn.LazyLinear(num_cells, device=device),
#     nn.Tanh(),
#     nn.LazyLinear(2 * env.action_spec.shape[-1], device=device),
#     NormalParamExtractor(),
#     )
#     policy_module = TensorDictModule(
#     actor_net, in_keys=["observation"], out_keys=["loc", "scale"]
#     )
#     policy_module = ProbabilisticActor(
#     module=policy_module,
#     spec=env.action_spec,
#     in_keys=["loc", "scale"],
#     distribution_class=TanhNormal,
#     # distribution_kwargs={
#     #     "min": env.action_spec.,
#     #     "max": env.action_spec.space.maximum,
#     # },
#     return_log_prob=True,
#     # we'll need the log-prob for the numerator of the importance weights
#     )
#     value_net = nn.Sequential(
#     nn.LazyLinear(num_cells, device=device),
#     nn.Tanh(),
#     nn.LazyLinear(num_cells, device=device),
#     nn.Tanh(),
#     nn.LazyLinear(num_cells, device=device),
#     nn.Tanh(),
#     nn.LazyLinear(1, device=device),
#     )

#     value_module = ValueOperator(
#         module=value_net,
#         in_keys=["observation"],
#     )
#     print("Running policy:", policy_module(env.reset()))
#     print("Running value:", value_module(env.reset()))
#     collector = SyncDataCollector(
#     env,
#     policy_module,
#     frames_per_batch=frames_per_batch,
#     total_frames=total_frames,
#     split_trajs=False,
#     device=device,
#     )
#     replay_buffer = ReplayBuffer(
#     storage=LazyTensorStorage(frames_per_batch),
#     sampler=SamplerWithoutReplacement(),
#     )
#     advantage_module = GAE(
#     gamma=gamma, lmbda=lmbda, value_network=value_module, average_gae=True
# )

#     loss_module = ClipPPOLoss(
#     actor=policy_module,
#     critic=value_module,
#     clip_epsilon=clip_epsilon,
#     entropy_bonus=bool(entropy_eps),
#     entropy_coef=entropy_eps,
#     # these keys match by default but we set this for completeness
#     value_target_key=advantage_module.value_target_key,
#     critic_coef=1.0,
#     gamma=0.99,
#     loss_critic_type="smooth_l1",
#     )